---
layout: episode
title:  "34: A big delta for Trino"
date: 2022-03-17
tags: delta-lake connector
youtube_id: "yp14n9g_-6s"
wistia_id: "hi9y3h0sl2"
sections:
   - title: "Releases 372, 373, and 374"
     desc: " Details about features and fixes in the new releases."
     time: 125
   - title: "Project Tardigrade update"
     desc: "Learn about our progress for fault-tolerant query processing."
     time: 561
   - title: "Concept of the episode"
     desc: "A new connector for Delta Lake object storage."
     time: 1117
   - title: "Pull requests of the episode"
     desc: "Add Delta Lake connector and documentation."
     time: 1570
   - title: "Demo of the episode"
     desc: "Delta Lake connector in action."
     time: 1754
   - title: "Question of the episode"
     desc: "How do I secure the connection from a Trino cluster to the data source?"
     time: 3240
---

## Guests

In this episode Manfred has the pleasure to chat with two colleagues, who
are working on making Trino better every day:

* [Claudius Li](https://github.com/claudiusli), Product Manager at Starburst
* [Joe Lodin](https://github.com/jhlodin), Information Engineer at Starburst

Brian is out to add another member to his family!

## Releases 372, 373, and 374

Official highlights from Martin Traverso:

[Trino 372](https://trino.io/docs/current/release/release-372.html)

* New `trim_array` function.
* Support for reading ZSTD-compressed Avro files.
* Support for column comments in Iceberg.
* Support for Kerberos authentication in Kudu connector.

[Trino 373](https://trino.io/docs/current/release/release-373.html)

* New Delta Lake connector.
* Improved performance of `LIKE` when querying Elasticsearch and PostgreSQL.
* Improved performance when querying partitioned Hive tables.
* Support access to S3 via HTTP proxy.

[Trino 374](https://trino.io/docs/current/release/release-374.html)

* Faster `GROUP BY` queries.
* Vim/Emacs editing mode for CLI.
* Support for `TRUNCATE TABLE` in Cassandra connector.
* Support `uint` types in ClickHouse.
* Support for Glue Metastore in Iceberg connector.
* Add `CREATE/DROP SCHEMA`, table and column comments in MongoDB
* Improved pushdown for PostgreSQL

Additional highlights from Manfred

* Timeout configuration for LDAP authentication.
* Values related to fault-tolerant execution in Web UI.
* JDBC `Driver.getProperties` enables more client applications like DBVisualizer.
* Vi and Emacs editing modes for interactive CLI usage.
* Performance improvements in PostgreSQL connector.
* SingleStore JDBC driver usage, end of `memsql` name.
* Documentation for the atop connector.

More detailed information is available in the
[Trino 372](https://trino.io/docs/current/release/release-372.html),
[Trino 373](https://trino.io/docs/current/release/release-373.html), and
[Trino 374](https://trino.io/docs/current/release/release-374.html) release
notes.

## Project Tardigrade update

The team around Project Tardigrade joined us in [episode 32](./32.html) to talk
about fault tolerant execution of queries in Trino. Now they have posted a
[status update on our blog]({%post_url 2022-02-16-tardigrade-project-update%}).

It looks like things are really coming along well, and Joe has joined the effort
to [create a first user-facing documentation
set](../docs/current/admin/fault-tolerant-execution.html).

The team has also posted a status update on the #project-tardigrade Slack
channel. Everything is ready for the community to perform first real world
testing, and help us make this a great feature set for Trino.

## Concept of the episode: A new connector for Delta Lake object storage

It is great to have a new connector in Trino, but what does that even mean?
Let's find out.

### What is a connector?

Just a quick refresher. Trino allows you to query many different data sources
with SQL statements. You enable that by creating a *catalog* that contains the
configuration to connect to a specific *data source*. The data source can be a
relational database, a NoSQL database, and an object storage. A *connector* is
the translation layer that maps the concepts in the data source to the Trino
concepts of schema, tables, rows, columns, data types and so on. The connector
needs to know how to retrieve the data itself from data source, and also how to
interact with the metadata.

Here are some examples metadata questions to answer:

* What are the available tables in schema `xyz`?
* What columns does table `abc` have and what are the data types?
* What file format is used by the storage for table `efg`?

And some queries about the actual data:

* Give me the top 100 rows from table `A`.
* Give me all files in partition `x` in the directory `y`.

So having a connector for your data source in Trino is a big deal. A connector
unlocks the data to all your SQL analytics powered by Trino, and the underlying
data source doesn't even have to support SQL.

### What is Delta Lake?

[Delta Lake](https://delta.io/) is an evolution of the Hive/Hadoop object
storage data source. It is an open-source storage format. Data is stored in
files, typically using binary formats such as Parquet or ORC. Metadata is stored
in a Hive Metastore Service (HMS).

Delta Lake supports ACID transactions, time travel, and many other features that
are lacking in the legacy Hive/Hadoop setup. This combination of traditional
data lake storage with data warehouse features is often called a lake house.

### History of the new connector

Delta Lake is fully open source, and part of the larger enterprise platform for
a lake house offered by [Databricks](https://databricks.com/).
[Starburst](https://www.starburst.io/) has supported Delta Lake users with a
connector for [Starburst Enterprise](https://docs.starburst.io/index.html#sep)
for nearly two years. To foster further adoption and innovation with the
community, the connector was [donated to Trino
373](https://docs.starburst.io/blog/2022-03-15-delta-lake.html) and continues to
be improved.

## Pull requests of the episode: Add Delta Lake connector and documentation

Over 25 developers helped [Jakob](https://github.com/jirassimok) with the effort
to [open-source the connector](https://github.com/trinodb/trino/pull/10897). It
is a heavy lift to migrate a such a full featured connectors into Trino. By
comparison the [documentation was
easy](https://github.com/trinodb/trino/pull/11229), but it is very important to
enable you. Well done everyone!

Let's have a look at the code in a bit more detail. A couple of key facts:

* The Delta Lake connector is just another plugin like all other connectors.
* This is a feature-rich connector supporting read and write operations.
* It shares implementation details with Hive and Iceberg connectors such as HMS
  access, Parquet and ORC file readers, and so on.

## Demo of the episode: Delta Lake connector in action

Now let's have a look at all this in action. In the demo Claudius uses
docker-compose to start up a HMS as metastore, MinIO as object storage, and of
course Trino as the query engine.

If you want to follow along, all resources used for the demo are [available on
our getting started
repository](https://github.com/bitsondatadev/trino-getting-started/tree/main/delta-lake).


Here is the sample catalog `delta.properties`:

```properties
connector.name=delta-lake
hive.metastore.uri=thrift://hive-metastore:9083
hive.s3.endpoint=http://minio:9000
hive.s3.aws-access-key=minio
hive.s3.aws-secret-key=minio123
hive.s3.path-style-access=true
delta.enable-non-concurrent-writes=true
```

Once everything is up and running we can start playing.

Verify that the catalog is available:

```sql
SHOW CATALOGS;
```

Check if there are any schemas:

```sql
SHOW SCHEMAS FROM delta;
```

Lets create a new schema:

```sql
CREATE SCHEMA delta.myschema WITH (location='s3a://claudiustestbucket/myschema');
```

Create a table, insert some records, and then verify:

```sql
CREATE TABLE delta.myschema.mytable (name varchar, id integer);
INSERT INTO delta.myschema.mytable VALUES ( 'John', 1), ('Jane', 2);
SELECT * FROM delta.myschema.mytable;
```

Run a query to get more data and insert it into a new table:

```sql
CREATE TABLE delta.myschema.myothertable AS
  SELECT * FROM delta.myschema.mytable;

SELECT * FROM delta.myschema.myothertable ;
```

Now for some data manipulation:

```sql
UPDATE delta.myschema.myothertable set name='Jonathan' where id=1;
SELECT * FROM delta.myschema.myothertable ;
DELETE FROM delta.myschema.myothertable where id=2;
SELECT * FROM delta.myschema.myothertable ;
```

And finally, lets clean up:

```sql
ALTER TABLE delta.myschema.mytable EXECUTE optimize(file_size_threshold => '10MB');
ANALYZE delta.myschema.myothertable;
DROP TABLE delta.myschema.myothertable ;
DROP TABLE delta.myschema.mytable ;
DROP SCHEMA delta.myschema;
```

As you can see with Trino and Delta Lake you get full create, read, update, and
delete operations on your lake house.

## Question of the episode: How do I secure the connection from a Trino cluster to the data source

Since we talked about connectors earlier, you already know that the
configuration for accessing a data source is assembled to create a catalog. This
approach uses a properties file in `etc/catalog`. For example, let's look at the
recently updated [SQL Server connector
documentation](../docs/current/connector/sqlserver.html):

```properties
connector.name=sqlserver
connection-url=jdbc:sqlserver://<host>:<port>;database=<database>;encrypt=false
connection-user=root
connection-password=secret
```

The connector uses username and password authentication. It connects using the
JDBC driver, which in turn enables TLs by default. A number of other connectors
also use JDBC drivers with username and password authentication, but the details
vary a lot. However, for all of them you can use [secrets support in
Trino](../docs/current/security/secrets.html) to use environment variable
references instead of hardcoding passwords.

When it comes to other connectors the details of securing a connection vary even
more. Ultimately the answer to how to secure the connection, and if that is even
possible, is the usual "It depends". Luckily you can check the documentation for
each connector to find out more and ping us on Slack if you need more help.

## Events, news, and various links

* [Starburst donates the Delta Lake connector to Trino](https://docs.starburst.io/blog/2022-03-15-delta-lake.html)
* [Operating Trino at Scale at Robinhood](https://www.meetup.com/trino-americas/events/282794002/)

Trino Meetup groups
 - Virtual
   - [Virtual Trino Americas](https://www.meetup.com/trino-americas/)
   - [Virtual Trino EMEA](https://www.meetup.com/trino-emea/)
   - [Virtual Trino APAC](https://www.meetup.com/trino-apac/)
 - East Coast (US)
   - [Trino Boston](https://www.meetup.com/trino-boston/)
   - [Trino NYC](https://www.meetup.com/trino-nyc/)
 - West Coast (US)
   - [Trino San Fransisco](https://www.meetup.com/trino-san-francisco/)
   - [Trino Los Angeles](https://www.meetup.com/trino-los-angeles/)
 - Mid West (US)
   - [Trino Chicago](https://www.meetup.com/trino-chicago/)

If you want to learn more about Trino, check out the definitive guide from
O'Reilly. You can download
[the free PDF](https://www.starburst.io/info/oreilly-trino-guide/) or
buy the book online.

Music for the show is from the [Megaman 6 Game Play album by Krzysztof
Słowikowski](https://krzysztofslowikowski.bandcamp.com/album/mega-man-6-gp).
